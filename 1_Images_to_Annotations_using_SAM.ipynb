{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a527a1d",
   "metadata": {},
   "source": [
    "## Libraries and SAM Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da6292-2e6f-46ce-88f1-cae3a1d92858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "import sys\n",
    "# !{sys.executable} -m pip install opencv-python matplotlib\n",
    "# !pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f1f75814-6366-4f36-8135-4797938147ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "051f98fc-5e63-4643-bc9c-07d3f77a17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ecea6",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "9987285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7144b29a-8430-4188-b2f4-4a3e5d986b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pltshow(masks,image,title='',annotations=False,size=5):\n",
    "    plt.figure(figsize=(size,size))\n",
    "    plt.imshow(image)\n",
    "    if(annotations):\n",
    "        show_anns(masks)\n",
    "    plt.axis('off')\n",
    "    if(title!=''):\n",
    "        plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "1fac8103-0c27-4ee3-b7e0-3bbe0f7db02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnMasks(image):\n",
    "    mask_generator_2 = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=20,\n",
    "    pred_iou_thresh=0.85,\n",
    "    stability_score_thresh=0.85,\n",
    "    min_mask_region_area=300,\n",
    "    stability_score_offset = 1.0,\n",
    "    box_nms_thresh = 0.7,\n",
    "    )\n",
    "    masks = mask_generator_2.generate(image)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "1ef013ba-97de-4f12-a15c-ec2103b15cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmask(greater,smaller):\n",
    "    return cv2.bitwise_and(greater,cv2.bitwise_not(smaller))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a386acff-9cc0-4e89-88b1-9f076fdcc3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskArea(mask):\n",
    "    return cv2.countNonZero(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749f565",
   "metadata": {},
   "source": [
    "## Removing small masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78833bc1",
   "metadata": {},
   "source": [
    "##### We remove small masks since there is a high probability of them being miscategorized. We then add all these small masks to the cell mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8a6758ea-8cd1-4454-bccd-3543ce469a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSmallMasks(masks,min_area):\n",
    "    topop=[]\n",
    "    for i,mask in enumerate(masks):\n",
    "        if maskArea(mask)<min_area:\n",
    "            topop.append(i)\n",
    "    topop.sort(reverse=True)\n",
    "    small_masks = []\n",
    "    for pos in topop:\n",
    "        small_masks.append(masks[pos])\n",
    "        del masks[pos]\n",
    "    return masks,small_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40493a8f",
   "metadata": {},
   "source": [
    "## Seperating overlapping area in different masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "6d47e75d-248f-4c93-906c-f19ab4ca3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disjoinMasks(masks):\n",
    "    newmasks=[]\n",
    "    for mask in masks:\n",
    "        newmasks.append((mask['segmentation'] * 255).astype(np.uint8))\n",
    "    for i,mask1 in enumerate(newmasks):      \n",
    "        \n",
    "        for j,mask2 in enumerate(newmasks):    \n",
    "                    \n",
    "            areaM1=maskArea(mask1)\n",
    "            areaM2=maskArea(mask2)\n",
    "            \n",
    "            if areaM1==areaM2:\n",
    "                continue\n",
    "            elif areaM1>areaM2:\n",
    "                newmasks[i]=cutmask(mask1,mask2) \n",
    "            elif areaM1<areaM2:\n",
    "               newmasks[j]=cutmask(mask2,mask1)\n",
    "    newmasks,small_masks=removeSmallMasks(newmasks,600)\n",
    "    return newmasks,small_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ea936-e540-48e8-9a4a-470d2a8a21d7",
   "metadata": {},
   "source": [
    "## Identifying and creating mask for bony-trabeculae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "b1cc8a2b-a1ea-40e7-82e3-1c46c430ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_bony_trabeculae(image,masksDisjoint,output_dir):\n",
    "    orange_lower = np.array([0, 50, 20], dtype=np.uint8)\n",
    "    orange_upper = np.array([100, 255, 255], dtype=np.uint8)\n",
    "    black_lower = np.array([0, 0, 0], dtype=np.uint8)\n",
    "    black_upper = np.array([30, 30, 30], dtype=np.uint8)\n",
    "    \n",
    "    accepted_masks_indices = []\n",
    "    combined_bony_mask = None\n",
    "    combined_bony_mask_binary = None\n",
    "    mask_number=0\n",
    "    bone_dir = output_dir+'/bony-trabeculae'\n",
    "    for mask_index, mask in enumerate(masksDisjoint):\n",
    "        result_image = cv2.bitwise_and(image, image, mask=mask) # superimposing otiginal colors on the mask\n",
    "        orange_mask = cv2.inRange(result_image, orange_lower, orange_upper)\n",
    "        black_mask = cv2.inRange(result_image, black_lower, black_upper)\n",
    "        no_pixels = np.sum(orange_mask > 0)\n",
    "        nb_pixels = np.sum(black_mask > 0)\n",
    "        height, width = image.shape[:2]\n",
    "        total_area = height * width\n",
    "        area_excluding_black = total_area - nb_pixels\n",
    "        result_image = cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if ((no_pixels / area_excluding_black) * 100) >= 45:\n",
    "            if combined_bony_mask is None:\n",
    "                combined_bony_mask = result_image\n",
    "                combined_bony_mask_binary = mask\n",
    "                cv2.imwrite(bone_dir+\"/mask-\"+str(mask_number)+\".png\",mask) # saving individual masks\n",
    "                mask_number+=1\n",
    "            else:\n",
    "                combined_bony_mask = cv2.bitwise_or(combined_bony_mask, result_image)\n",
    "                combined_bony_mask_binary = cv2.bitwise_or(combined_bony_mask_binary, mask)\n",
    "                cv2.imwrite(bone_dir+\"/mask-\"+str(mask_number)+\".png\",mask) # saving individual masks\n",
    "                mask_number+=1\n",
    "\n",
    "            accepted_masks_indices.append(mask_index)\n",
    "    combined_bony_mask_copy = combined_bony_mask.copy()\n",
    "    combined_bony_mask_copy = cv2.cvtColor(combined_bony_mask_copy, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imwrite(output_dir+\"/bone-mask\"+\".tif\",combined_bony_mask_copy) # saving combined masks\n",
    "    # Removing masks from masksDisjoint that have been identified as fat.\n",
    "    for index in sorted(accepted_masks_indices, reverse=True):\n",
    "        del masksDisjoint[index]\n",
    "\n",
    "    return masksDisjoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ac9f9-2296-421b-a1d5-58486900dfa6",
   "metadata": {},
   "source": [
    "## Identifying and creating mask for fat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e7a99803-e4b8-4dba-8380-24fa5928f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_fat(image,masksDisjoint,output_dir):\n",
    "    black_lower = np.array([0, 0, 0], dtype=np.uint8)\n",
    "    black_upper = np.array([30, 30, 30], dtype=np.uint8)\n",
    "    accepted_masks_indices = []\n",
    "    combined_fat_mask = None\n",
    "    combined_fat_mask_binary = None\n",
    "    mask_number=0\n",
    "    fat_dir = output_dir+'/fat'\n",
    "    for mask_index,mask in enumerate(masksDisjoint):\n",
    "        result_image = cv2.bitwise_and(image, image, mask=mask) # superimposing otiginal colors on the mask\n",
    "        black_mask = cv2.inRange(result_image, black_lower, black_upper)\n",
    "       \n",
    "        # Invert the black mask to exclude black pixels\n",
    "        non_black_mask = cv2.bitwise_not(black_mask)\n",
    "       \n",
    "        # Compute mean and standard deviation of pixel intensities excluding black pixels\n",
    "        mean, std_devs = cv2.meanStdDev(result_image, mask=non_black_mask)\n",
    "       \n",
    "        # Flatten the array of standard deviations\n",
    "        std_devs = std_devs.flatten()\n",
    "       \n",
    "        # Compute the average standard deviation\n",
    "        average_std_dev = np.mean(std_devs)\n",
    "        if average_std_dev < 20:\n",
    "            # Combine the selected masks into one mask using bitwise OR\n",
    "            if combined_fat_mask is None:\n",
    "                combined_fat_mask = result_image\n",
    "                combined_fat_mask_binary = mask\n",
    "                cv2.imwrite(fat_dir+\"/mask-\"+str(mask_number)+\".png\",mask) # saving individual masks\n",
    "                mask_number += 1\n",
    "            else:\n",
    "                combined_fat_mask = cv2.bitwise_or(combined_fat_mask, result_image)\n",
    "                combined_fat_mask_binary = cv2.bitwise_or(combined_fat_mask_binary, mask)\n",
    "                cv2.imwrite(fat_dir+\"/mask-\"+str(mask_number)+\".png\",mask) # saving individual masks\n",
    "                mask_number += 1\n",
    "            accepted_masks_indices.append(mask_index)\n",
    "    \n",
    "\n",
    "    cv2.imwrite(output_dir+\"/fat-mask\"+\".tif\",combined_fat_mask) # saving combined masks\n",
    "    # Removing masks from masksDisjoint that have been identified as fat.\n",
    "    for index in sorted(accepted_masks_indices, reverse=True):\n",
    "        del masksDisjoint[index]\n",
    "\n",
    "    return masksDisjoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d9ce88-e8cc-49da-8f3f-3a84b16aa06b",
   "metadata": {},
   "source": [
    "## Creating mask for cellular area(including reticulin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "ff6aee9c-7ae5-49d3-9146-36c64bab2b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_cell(image,masksDisjoint,output_dir):\n",
    "    combined_cell_mask = None\n",
    "    cell_dir = output_dir + \"/cell\"\n",
    "    mask_number = 0\n",
    "    for mask in masksDisjoint:\n",
    "        result_image = cv2.bitwise_and(image, image, mask=mask) # superimposing otiginal colors on the mask\n",
    "        result_image = cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB)\n",
    "        if combined_cell_mask is None:\n",
    "            combined_cell_mask = result_image\n",
    "            cv2.imwrite(cell_dir+\"/mask-\"+str(mask_number)+\".png\",mask) # saving individual masks\n",
    "            mask_number += 1\n",
    "        else:\n",
    "            combined_cell_mask = cv2.bitwise_or(combined_cell_mask, result_image) # saving individual masks\n",
    "            cv2.imwrite(cell_dir+\"/mask-\"+str(mask_number)+\".png\",mask)\n",
    "            mask_number += 1\n",
    "    \n",
    "    cv2.imwrite(output_dir+\"/cell-mask\"+\".tif\",combined_cell_mask) # saving combined masks\n",
    "    return combined_cell_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b34a97fd-6e9b-4144-9c12-944289ce2d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_elongation(rect):\n",
    "            \"\"\"Calculate elongation from rotated rectangle.\"\"\"\n",
    "            width, height = rect[1]\n",
    "            if width == 0 or height == 0:\n",
    "                return 0  # Return 0 elongation if width or height is zero\n",
    "            else:\n",
    "                return 1 - (min(height, width) / max(height, width))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e96a2",
   "metadata": {},
   "source": [
    "## Conversion from images to classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce1ec6",
   "metadata": {},
   "source": [
    "##### Once we have images that require automatic annotation, SAM segments these images into regions using binary masks known as 'segmentation objects.' Each mask represents a region identified by SAM. However, these masks do not inherently classify regions into specific categories. To achieve meaningful segmentation, we need to assign class labels to these regions based on their characteristics. Let's examine how SAM converts images into segmentation objects and how we can classify the masks based on their unique characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "8b5a8394-695c-4b2f-8577-de2b8138431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_directory_path=\"IMAGES\" #input direcory contating images\n",
    "output_directory_path = \"masks\" #output directory that will contain classified segmentations\n",
    "for n, imagefile in enumerate(os.listdir(input_directory_path)):\n",
    "    image_path = os.path.join(input_directory_path, imagefile)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    image_name=imagefile.split(\".\")[0]\n",
    "    output_dir=output_directory_path+'/'+image_name\n",
    "    \n",
    "\n",
    "    # Our annotations will first be saved in a folder annotations for all images. The structure of this folder will be:\n",
    "    #Image Name\n",
    "     #-->bony-trabeculae\n",
    "     #   /-->mask-0.png\n",
    "     #   /-->mask-1.png\n",
    "     #\n",
    "     #fat\n",
    "     #   /-->mask-0.png\n",
    "     #   /-->mask-1.\n",
    "     #\n",
    "     #cell\n",
    "     #   /-->mask-0.png\n",
    "     #   /-->mask-1.png\n",
    "     #\n",
    "     #reticulin\n",
    "     #   /-->mask.png\n",
    "     #\n",
    "     #-->bone-mask.tif\n",
    "     #-->fat-mask.tif\n",
    "     #-->cell-mask.tif\n",
    "     #-->original-image.tif\n",
    "\n",
    "\n",
    "   \n",
    "    os.mkdir(output_dir)\n",
    "    os.mkdir(output_dir+'/bony-trabeculae')\n",
    "    os.mkdir(output_dir+'/reticulin')\n",
    "    os.mkdir(output_dir+'/fat')\n",
    "    os.mkdir(output_dir+'/cell')\n",
    "    cv2.imwrite(output_dir+\"/original-image\"+\".tif\",image) # saving the original image\n",
    "\n",
    "    masks = returnMasks(image) # generating masks using SAM, the output is a list of binary image with white region as the segmented area/foreground and black being the rest of the image/background\n",
    "\n",
    "    \n",
    "\n",
    "    masksDisjoint,small_masks = disjoinMasks(masks)\n",
    "    masksDisjoint = identify_bony_trabeculae(image, masksDisjoint,output_dir)\n",
    "    \n",
    "    masksDisjoint = identify_fat(image,masksDisjoint,output_dir)\n",
    "    masksDisjoint = np.concatenate((masksDisjoint,small_masks),axis=0)\n",
    "    combined_cell_mask = identify_cell(image,masksDisjoint,output_dir)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33823f38",
   "metadata": {},
   "source": [
    "##### Now, by taking a look at the combined masks of each class for every image, we ensure that the classification is correct. If there is a misclassification, we move that individual mask to the directory of the correct class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "04927267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineBone(dir):\n",
    "    combined_mask = None\n",
    "    for n,file_name in enumerate(os.listdir(dir)):\n",
    "        mask_path = os.path.join(dir, file_name)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if combined_mask is None:\n",
    "            \n",
    "            combined_mask = mask\n",
    "        else:\n",
    "            combined_mask = cv2.bitwise_or(combined_mask,mask)\n",
    "\n",
    "    shutil.rmtree(dir)\n",
    "    os.mkdir(dir)\n",
    "    if combined_mask is not None:\n",
    "        cv2.imwrite(dir+\"/bone-mask\"+\".png\",combined_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "a882f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineFat(dir):\n",
    "    combined_mask = None\n",
    "    for n,file_name in enumerate(os.listdir(dir)):\n",
    "        mask_path = os.path.join(dir, file_name)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if combined_mask is None:\n",
    "            combined_mask = mask\n",
    "        else:\n",
    "            combined_mask = cv2.bitwise_or(combined_mask,mask)\n",
    "\n",
    "    shutil.rmtree(dir)\n",
    "    os.mkdir(dir)\n",
    "    if combined_mask is not None:\n",
    "        cv2.imwrite(dir+\"/fat-mask\"+\".png\",combined_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "399bdc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineCell(dir,b_dir):\n",
    "    combined_mask = None\n",
    "    for n,file_name in enumerate(os.listdir(dir)):\n",
    "        mask_path = os.path.join(dir, file_name)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if combined_mask is None:\n",
    "            combined_mask = mask\n",
    "        else:\n",
    "            combined_mask = cv2.bitwise_or(combined_mask,mask)\n",
    "\n",
    "    plt.imshow(combined_mask,cmap=\"gray\")\n",
    "    plt.show()\n",
    "    shutil.rmtree(dir)\n",
    "    os.mkdir(dir)\n",
    "    if combined_mask is not None:\n",
    "        cv2.imwrite(dir+\"/cell-mask\"+\".png\",combined_mask)\n",
    "\n",
    "\n",
    "    # in some cases it may be that sam was unable to generate mask for cell, in that case we combine bone and fat masks and invert them to get the cell mask\n",
    "    # this is not required for every image, so if not required comment out\n",
    "    # Load the original image to get its dimensions\n",
    "    og = cv2.imread(b_dir+\"/original-image.tif\", cv2.IMREAD_GRAYSCALE)\n",
    "      \n",
    "    # Create bone and fat masks\n",
    "    bone = np.zeros_like(og)\n",
    "    fat = np.zeros_like(og)\n",
    "\n",
    "    # Check if bone mask exists, if not, it will remain all black\n",
    "    bone_path = b_dir + \"/bony-trabeculae/bone-mask.png\"\n",
    "\n",
    "    if os.path.exists(bone_path):\n",
    "        bone = cv2.imread(bone_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Check if fat mask exists, if not, it will remain all black\n",
    "    fat_path = b_dir + \"/fat/fat-mask.png\"\n",
    "    \n",
    "    if os.path.exists(fat_path):\n",
    "        fat = cv2.imread(fat_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "    total_cell = cv2.bitwise_or(bone,fat)\n",
    "    total_cell = cv2.bitwise_not(total_cell)\n",
    "    cv2.imwrite(dir+\"/cell-mask-2\"+\".png\",total_cell)\n",
    "\n",
    "    # we now manually verify the two cell masks and delete one of them to avoid repition. If one of them is incorrect then we delete the incorrect mask\n",
    "    # we rename this mask as cell-mask.png\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157dc19",
   "metadata": {},
   "source": [
    "## Combining the individual masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c4d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Masks = \"masks\" # input directory\n",
    "for n,dir in enumerate(os.listdir(Masks)):\n",
    "    subdir_path = os.path.join(Masks, dir)\n",
    "\n",
    "    for image_folder in os.listdir(subdir_path):\n",
    "        f_dir = os.path.join(subdir_path, image_folder)\n",
    "        if image_folder == \"bony-trabeculae\":\n",
    "            if len(os.listdir(f_dir)) != 0:\n",
    "                combineBone(f_dir)\n",
    "            \n",
    "        elif image_folder == \"fat\":\n",
    "            if len(os.listdir(f_dir)) != 0:\n",
    "                combineFat(f_dir)\n",
    "    f_dir = os.path.join(subdir_path, \"cell\")\n",
    "\n",
    "    if len(os.listdir(f_dir)) != 0:\n",
    "        combineCell(f_dir,subdir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f0b05",
   "metadata": {},
   "source": [
    "## Isolating Reticulin Fibers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "8b8a8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_reticulin(dir):\n",
    "    mask = os.listdir(dir+\"/cell\")\n",
    "    print(mask)\n",
    "    cell_mask = cv2.imread(dir+\"/cell/\"+mask[0]) \n",
    "    print(dir+\"/cell/\"+mask[0])\n",
    "    print(os.path.exists(dir+\"/\"+mask[0]))\n",
    "    plt.imshow(cell_mask,cmap=\"gray\")\n",
    "    plt.show()\n",
    "    # o_ret = cv2.imread(dir+\"/reticulin/mask.png\") \n",
    "    o_img = cv2.imread(dir+\"/original-image.tif\")\n",
    "    bone = np.zeros((cell_mask.shape[0], cell_mask.shape[1]), dtype=cell_mask.dtype)\n",
    "    fat = np.zeros((cell_mask.shape[0], cell_mask.shape[1]), dtype=cell_mask.dtype)\n",
    "\n",
    "    o_img = cv2.cvtColor(o_img, cv2.COLOR_BGR2RGB)\n",
    "    # Check if bone mask exists, if not, it will remain all black\n",
    "    bone_path = dir + \"/bony-trabeculae/bone-mask.png\"\n",
    "    if os.path.exists(bone_path):\n",
    "        bone = cv2.imread(bone_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "    # Check if fat mask exists, if not, it will remain all black\n",
    "    fat_path = dir + \"/fat/fat-mask.png\"\n",
    "    if os.path.exists(fat_path):\n",
    "        fat = cv2.imread(fat_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    b_mask = cv2.bitwise_not(bone)\n",
    "    fat_mask = cv2.bitwise_not(fat)\n",
    "    b_mask = cv2.bitwise_or(bone,fat)\n",
    "\n",
    "    b_mask = cv2.bitwise_not(b_mask)\n",
    "    test = o_img\n",
    "    # as the cell mask will be black in places where bone and fat are present, so we need to convert that area into white before thresholding\n",
    "    test[b_mask == 0] = 255,255,255\n",
    "\n",
    "    grayscale_image = cv2.cvtColor(test, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "\n",
    "    # Apply manual binary thresholding\n",
    "    # Adjust the value of the threshold to ensure maximum reticulin identification \n",
    "    _, binary_image = cv2.threshold(grayscale_image, 155, 255, cv2.THRESH_BINARY)\n",
    "    binary_image= cv2.bitwise_not(binary_image)\n",
    "\n",
    "    # Find contours in the binary image\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Create a copy of the thresholded image for drawing contours\n",
    "    contours_image =cv2.bitwise_not(binary_image)\n",
    "\n",
    "    contours_image = cv2.cvtColor(contours_image,cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Convert BGR to RGB\n",
    "    contours_image_rgb = cv2.cvtColor(contours_image, cv2.COLOR_BGR2RGB)    \n",
    "\n",
    "    # Display using matplotlib\n",
    "    # plt.figure(figsize=(10, 8))  # Adjust figure size as needed\n",
    "    \n",
    "    accepted_contours = []\n",
    "    rejected_contours = []\n",
    "\n",
    "    # Loop over the contours\n",
    "    for contour in contours:\n",
    "        # Fit a rotated rectangle around the contour\n",
    "        rect = cv2.minAreaRect(contour)\n",
    "        \n",
    "        # Calculate elongation\n",
    "        elongation = calculate_elongation(rect)       \n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "        if elongation >= 0.25:\n",
    "            accepted_contours.append(contour)\n",
    "        else:\n",
    "            rejected_contours.append(contour)\n",
    "    \n",
    "        \n",
    "\n",
    "    # Create masks for accepted and rejected contours\n",
    "    accepted_contours_mask = np.zeros_like(binary_image)\n",
    "    rejected_contours_mask = np.zeros_like(binary_image)\n",
    "\n",
    "    # Draw contours onto masks\n",
    "    for contour in accepted_contours:\n",
    "        cv2.drawContours(accepted_contours_mask, [contour], 0, (255), cv2.FILLED)\n",
    "\n",
    "    for contour in rejected_contours:\n",
    "        cv2.drawContours(rejected_contours_mask, [contour], 0, (255), cv2.FILLED)\n",
    "\n",
    "    # Invert the rejected contours mask\n",
    "    rejected_contours_mask = cv2.bitwise_not(rejected_contours_mask)\n",
    "    \n",
    "    # Perform bitwise AND operation with the binary image and rejected contours mask\n",
    "    final_image = cv2.bitwise_and(binary_image, binary_image, mask=rejected_contours_mask)\n",
    "    tem = cv2.bitwise_not(final_image)\n",
    "\n",
    "    # # Plot both images side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    o_img = cv2.imread(dir+\"/original-image.tif\")\n",
    "    o_img = cv2.cvtColor(o_img, cv2.COLOR_BGR2RGB)\n",
    "    # Plot the original image\n",
    "    axes[0].imshow(o_img)\n",
    "    axes[0].set_title('Original Image')\n",
    "\n",
    "    # Plot the thresholded image\n",
    "    axes[1].imshow(tem,cmap=\"gray\")\n",
    "    axes[1].set_title('Threshold Image')\n",
    "    # Hide the axes\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    shutil.rmtree(dir+\"/reticulin\")\n",
    "    os.mkdir(dir+\"/reticulin\")\n",
    "    cv2.imwrite(dir+\"/reticulin/mask.png\",final_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4e022-65cf-42ce-838f-efb08e56590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Masks = \"masks\" #Directory containing all masks\n",
    "for n, dir in enumerate(os.listdir(Masks)):\n",
    "    subdir_path = os.path.join(Masks, dir)\n",
    "    identify_reticulin(subdir_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33600a6",
   "metadata": {},
   "source": [
    "## Converting masks to annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0612070",
   "metadata": {},
   "source": [
    "##### We use an annotation platform called CVAT to edit our annotations. This platform requires annotations in a standard format such as PascalVOC, COCO, or Cityscapes. PascalVOC is an annotation format which is popular and easy to manipulate, as it saves annotations in the form of images, rather than boundary pixel coordinates in XML files or JSON files used by the COCO format. The following is the directory structure of PascalVOC:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d3fc4",
   "metadata": {},
   "source": [
    "- Annotations\n",
    "    - Image_Name_1.xml\n",
    "    - Image_Name_2.xml\n",
    "- ImageSets\n",
    "    - Main\n",
    "        - default.txt\n",
    "    - Segmentation\n",
    "        - default.txt\n",
    "- JPEGImages (optional)\n",
    "    - Image_Name_1.jpg\n",
    "    - Image_Name_2.jpg        \n",
    "- SegmentationClass\n",
    "    - Image_Name_1.jpg\n",
    "    - Image_Name_2.jpg\n",
    "- SegmentationObject\n",
    "    - Image_Name_1.png\n",
    "    - Image_Name_2.png\n",
    "- lablmap.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0d40b",
   "metadata": {},
   "source": [
    "Contents of each file/folder\n",
    "- Annotations: Contains XML files that provide basic information for each image, such as filename, width, and height.\n",
    "- ImageSets: Contains two TXT files under Main and Segmentation. These files list the names of each image.\n",
    "- JPEGImages (optional): This folder contains all the original images.\n",
    "- SegmentationClass: A folder containing PNG files of SegmentationClass images. Each pixel in these images represents a class with a specific color. For example, cells, background (bg), bone, and fat each have distinct colors. There are only four colors in these images.\n",
    "- SegmentationObject: A folder containing PNG files of SegmentationObject images. Each distinct object recognized in the images is assigned a different color. For instance, fat1, fat2, bone1, and bone2 are different objects within their respective classes, each colored uniquely. These images can use up to 256 different colors, the maximum limit of PascalVOC.\n",
    "- lablmap.txt: This file specifies the color of each class in SegmentationClass. For example, fat:0,128,0 corresponds to the color representation of cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4e95f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "37899e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the Xml file like the following example:\n",
    "#     <annotation>\n",
    "#        <folder/>\n",
    "#            <filename>PGI_image_0.jpg</filename>\n",
    "#        <size>\n",
    "#            <width>1200</width>\n",
    "#            <height>1920</height>\n",
    "#            <depth/>\n",
    "#        </size>\n",
    "#        <segmented>1</segmented>\n",
    "#     </annotation>5\n",
    "\n",
    "\n",
    "def makeXML(img, width, height):\n",
    "    annotation = etree.Element('annotation')\n",
    "\n",
    "    fo = etree.Element('folder')\n",
    "    fo.text =  ''\n",
    "\n",
    "    annotation.append(fo)\n",
    "\n",
    "    f = etree.Element('filename')\n",
    "    f.text = str(img+'.jpg')\n",
    "\n",
    "    annotation.append(f)\n",
    "\n",
    "    size = etree.Element('size')\n",
    "    w = etree.Element('width')\n",
    "    w.text = str(width)\n",
    "    h = etree.Element('height')\n",
    "    h.text = str(height)\n",
    "    d = etree.Element('depth')\n",
    "    d.text = str('')\n",
    "\n",
    "\n",
    "    size.append(w)\n",
    "    size.append(h)\n",
    "    size.append(d)\n",
    "    annotation.append(size)\n",
    "\n",
    "    segmented=etree.Element('segmented')\n",
    "    segmented.text='1'\n",
    "    annotation.append(segmented)\n",
    "\n",
    "\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "5e14a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanOutput='./masks' # input the folder where we saved our annotations  \n",
    "OutputDir='PASCALoutput/' # Folder that will contain the PascalVOC annotation\n",
    "imagesFolder='IMAGES/' # Folder that will contains only images\n",
    "\n",
    "# Create the directories\n",
    "os.makedirs(imagesFolder, exist_ok=True)\n",
    "os.makedirs(os.path.join(OutputDir, 'SegmentationObject'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OutputDir, 'SegmentationClass'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OutputDir, 'Annotations'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OutputDir, 'ImageSets', 'Main'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OutputDir, 'ImageSets', 'Segmentation'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "71590da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_color={\n",
    "    'bony-trabeculae':[128,128,0],\n",
    "    'cell':[128,0,0],\n",
    "    'reticulin':[0,0,128],\n",
    "    'fat':[0,128,0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "db77ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "default=\"\" #default.txt file in 'ImageSets/Main' and 'ImageSets/Segmentation'\n",
    "\n",
    "#EDIT THIS TO BE IN COORDINATION WITH class_color\n",
    "labelmap=\"# label:color_rgb:parts:actions\\nbackground:0,0,0::\\ncell:128,0,0::\\nfat:0,128,0::\\nreticulin:0,0,128::\\nbony-trabeculae:128,128,0::\" #labelmap.txt file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "4fb1b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_for_pascal(rgb_image):\n",
    "    gray_image = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_mask = cv2.threshold(gray_image, 15, 255, cv2.THRESH_BINARY)\n",
    "    return binary_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "bd672052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_map_get256(N=256):\n",
    "    def bitget(byteval, idx):\n",
    "        return ((byteval & (1 << idx)) != 0)\n",
    "\n",
    "    cmap = np.zeros((N, 3), dtype=np.dtype)\n",
    "    for i in range(N):\n",
    "        r = g = b = 0\n",
    "        c = i\n",
    "        for j in range(8):\n",
    "            r = r | (bitget(c, 0) << 7-j)\n",
    "            g = g | (bitget(c, 1) << 7-j)\n",
    "            b = b | (bitget(c, 2) << 7-j)\n",
    "            c = c >> 3\n",
    "\n",
    "        cmap[i] = np.array([r, g, b])\n",
    "    return cmap\n",
    "\n",
    "color_map256=color_map_get256()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "b3e7daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addObject2Image(image,objects,color): # add an object to image with random color\n",
    "    masked_image=image.copy()\n",
    "    masked_image[objects==255]=color[::-1]\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addClass2Image(image,mask,color_rgb):\n",
    "    color_bgr=color_rgb[::-1]\n",
    "    masked_image=image.copy()\n",
    "    masked_image[mask==255]=color_bgr\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "default=\"\" #making sure default file is empty before appending\n",
    "outofLimitMaskClasses=[]\n",
    "for n,dir in enumerate(os.listdir(cleanOutput)): #dir=MQ_image_0 or PGI_image_0\n",
    "        # print(dir)\n",
    "        default+=dir+\"\\n\" #adding image name to default.txt\n",
    "\n",
    "        og_img=cv2.imread(cleanOutput+'/'+dir+'/original-image.tif')\n",
    "        cv2.imwrite(imagesFolder+'/'+dir+'.tif',og_img) #adding image to image directory for data to be uploaded later\n",
    "\n",
    "        #making annotation xml file for each image\n",
    "        og_image_shape=og_img.shape\n",
    "        annotation=makeXML(dir,og_image_shape[0],og_image_shape[1])\n",
    "        save_path = os.path.join(OutputDir+'/Annotations/'+dir+ \".xml\")\n",
    "        with open(save_path, 'wb') as file:\n",
    "            aux = etree.tostring(annotation, pretty_print=True)\n",
    "            aux.decode(\"utf-8\")\n",
    "            file.write(aux)\n",
    "\n",
    "        image_objects=np.zeros(og_img.shape) #initiallly we use empty mask before overlaying individual masks on it\n",
    "        image_classes=np.zeros(og_img.shape) #initiallly we use empty mask before overlaying combined class mask on it\n",
    "        empty_mask=np.zeros_like(cv2.cvtColor(og_img,cv2.COLOR_BGR2GRAY)) #to save combined mask for each class\n",
    "\n",
    "        combined_class={\n",
    "            'bony-trabeculae': empty_mask,\n",
    "            'reticulin': empty_mask,\n",
    "            'fat': empty_mask,\n",
    "            'cell': empty_mask\n",
    "        }\n",
    "\n",
    "        color_number_for_mask=2 #start colouring masks from color no. 2 in color_map256\n",
    "\n",
    "        for item in os.listdir(cleanOutput+'/'+dir): # bone,cell,og.jpg\n",
    "            item_path = os.path.join(cleanOutput+'/'+dir, item) # '/Individual-Masks/MQ_image_0/bone'\n",
    "            if os.path.isdir(item_path): \n",
    "                mask = os.listdir(item_path)\n",
    "                if mask:\n",
    "\n",
    "                    mask=cv2.imread(item_path+'/'+mask[0])\n",
    "                    binary_mask=binarize_for_pascal(mask)\n",
    "\n",
    "                    combined_class[item]=cv2.bitwise_or(combined_class[item],binary_mask)  #combine each class's mask\n",
    "\n",
    "                    image_objects=addObject2Image(image_objects,binary_mask,color_map256[color_number_for_mask])\n",
    "                    \n",
    "                    color_number_for_mask+=1\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "        for mask_class in ['cell','reticulin','fat','bony-trabeculae']:\n",
    "            image_classes=addClass2Image(image_classes,combined_class[mask_class],class_color[mask_class])\n",
    "\n",
    "        cv2.imwrite(OutputDir+'/SegmentationObject/'+dir+'.png',image_objects)\n",
    "        cv2.imwrite(OutputDir+'/SegmentationClass/'+dir+'.png',image_classes)\n",
    "\n",
    "        print(n,end=\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "1e5f6288",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(OutputDir+'/ImageSets/Main/default.txt', \"w\")\n",
    "text_file.write(default)\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(OutputDir+'/ImageSets/Segmentation/default.txt', \"w\")\n",
    "text_file.write(default)\n",
    "text_file.close()\n",
    "\n",
    "text_file = open(OutputDir+'/labelmap.txt', \"w\")\n",
    "text_file.write(labelmap)\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
